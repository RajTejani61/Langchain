{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc80109",
   "metadata": {},
   "outputs": [],
   "source": [
    "from yt_dlp import YoutubeDL\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9da3a9cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_classic.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough, RunnableLambda\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dddcd42",
   "metadata": {},
   "source": [
    "## Indexing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "099a2d40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: [youtube] Falling back to generic n function search\n",
      "         player = https://www.youtube.com/s/player/7dc3db36/player_ias.vflset/en_US/base.js\n",
      "WARNING: [youtube] LPZh9BOjkQs: nsig extraction failed: Some formats may be missing\n",
      "         n = 7pby3wnH3Y1MxdaIq4 ; player = https://www.youtube.com/s/player/7dc3db36/player_ias.vflset/en_US/base.js\n",
      "         Please report this issue on  https://github.com/yt-dlp/yt-dlp/issues?q= , filling out the appropriate issue template. Confirm you are on the latest version using  yt-dlp -U\n",
      "WARNING: [youtube] LPZh9BOjkQs: Some web_safari client https formats have been skipped as they are missing a url. YouTube is forcing SABR streaming for this client. See  https://github.com/yt-dlp/yt-dlp/issues/12482  for more details\n",
      "WARNING: [youtube] LPZh9BOjkQs: Some web client https formats have been skipped as they are missing a url. YouTube is forcing SABR streaming for this client. See  https://github.com/yt-dlp/yt-dlp/issues/12482  for more details\n",
      "WARNING: ffmpeg not found. The downloaded format may not be the best available. Installing ffmpeg is strongly recommended: https://github.com/yt-dlp/yt-dlp#dependencies\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Transcript fetched successfully!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "video_url = \"https://www.youtube.com/watch?v=LPZh9BOjkQs\"\n",
    "\n",
    "ydl_opts = {\n",
    "    \"skip_download\": True,        # don't download video\n",
    "    \"writesubtitles\": True,       # write subtitles\n",
    "    \"writeautomaticsub\": True,    # also include auto-generated subtitles\n",
    "    \"subtitlesformat\": \"vtt\",     # format: .vtt or .srt\n",
    "    \"subtitleslangs\": [\"en\"],     # select language\n",
    "    \"quiet\": True,\n",
    "}\n",
    "\n",
    "with YoutubeDL(ydl_opts) as ydl:\n",
    "    info = ydl.extract_info(video_url, download=False)\n",
    "    subtitles = info.get(\"subtitles\") or info.get(\"automatic_captions\")\n",
    "\n",
    "    if not subtitles:\n",
    "        print(\"❌ No subtitles found.\")\n",
    "    else:\n",
    "        # pick first English subtitle\n",
    "        subtitle_url = subtitles[\"en\"][0][\"url\"]\n",
    "        import requests\n",
    "        response = requests.get(subtitle_url)\n",
    "        if response.ok:\n",
    "            print(\"✅ Transcript fetched successfully!\\n\")\n",
    "            # print(response.text)\n",
    "        else:\n",
    "            print(\"⚠️ Failed to download subtitle content.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55124058",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"wireMagic\": \"pb3\",\n",
      "  \"pens\": [ {\n",
      "  \n",
      "  } ],\n",
      "  \"wsWinStyles\": [ {\n",
      "  \n",
      "  } ],\n",
      "  \"wpWinPositions\": [ {\n",
      "  \n",
      "  } ],\n",
      "  \"events\": [ {\n",
      "    \"tStartMs\": 1140,\n",
      "    \"dDurationMs\": 2836,\n",
      "    \"segs\": [ {\n",
      "      \"utf8\": \"Imagine you happen across a short movie script that\"\n",
      "    } ]\n",
      "  }, {\n",
      "    \"tStartMs\": 3976,\n",
      "    \"dDurationMs\": 3164,\n",
      "    \"segs\": [ {\n",
      "      \"utf8\": \"describes a scene between a person and their AI assistant.\"\n",
      "    } ]\n",
      "  }, {\n",
      "    \"tStartMs\": 7480,\n",
      "    \"dDurationMs\": 5580,\n",
      "    \"segs\": [ {\n",
      "      \"ut\n"
     ]
    }
   ],
   "source": [
    "print(response.text[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46456004",
   "metadata": {},
   "outputs": [],
   "source": [
    "transcript = json.loads(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "424c849d",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_list = [\n",
    "    seg[\"utf8\"]\n",
    "    for event in transcript.get(\"events\", [])\n",
    "        if \"segs\" in event\n",
    "        for seg in event[\"segs\"]\n",
    "            if \"utf8\" in seg\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f85fb040",
   "metadata": {},
   "outputs": [],
   "source": [
    "transcript = [\"\\n\".join(text_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8b2b7f04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Imagine you happen across a short movie script that\\ndescribes a scene between a person and their AI assistant.\\nThe script has what the person asks the AI, but the AI's response has been torn off.\\nSuppose you also have this powerful magical machine that can take\\nany text and provide a sensible prediction of what word comes next.\\nYou could then finish the script by feeding in what you have to the machine,\\nseeing what it would predict to start the AI's answer,\\nand then repeating this over and over with a growing script completing the dialogue.\\nWhen you interact with a chatbot, this is exactly what's happening.\\nA large language model is a sophisticated mathematical function\\nthat predicts what word comes next for any piece of text.\\nInstead of predicting one word with certainty, though,\\nwhat it does is assign a probability to all possible next words.\\nTo build a chatbot, you lay out some text that describes an interaction between a user\\nand a hypothetical AI assistant, add on whatever the user types in as the first part of\\nthe interaction, and then have the model repeatedly predict the next word that such a\\nhypothetical AI assistant would say in response, and that's what's presented to the user.\\nIn doing this, the output tends to look a lot more natural if\\nyou allow it to select less likely words along the way at random.\\nSo what this means is even though the model itself is deterministic,\\na given prompt typically gives a different answer each time it's run.\\nModels learn how to make these predictions by processing an enormous amount of text,\\ntypically pulled from the internet.\\nFor a standard human to read the amount of text that was used to train GPT-3,\\nfor example, if they read non-stop 24-7, it would take over 2600 years.\\nLarger models since then train on much, much more.\\nYou can think of training a little bit like tuning the dials on a big machine.\\nThe way that a language model behaves is entirely determined by these\\nmany different continuous values, usually called parameters or weights.\\nChanging those parameters will change the probabilities\\nthat the model gives for the next word on a given input.\\nWhat puts the large in large language model is how\\nthey can have hundreds of billions of these parameters.\\nNo human ever deliberately sets those parameters.\\nInstead, they begin at random, meaning the model just outputs gibberish,\\nbut they're repeatedly refined based on many example pieces of text.\\nOne of these training examples could be just a handful of words,\\nor it could be thousands, but in either case, the way this works is to\\npass in all but the last word from that example into the model and\\ncompare the prediction that it makes with the true last word from the example.\\nAn algorithm called backpropagation is used to tweak all of the parameters\\nin such a way that it makes the model a little more likely to choose\\nthe true last word and a little less likely to choose all the others.\\nWhen you do this for many, many trillions of examples,\\nnot only does the model start to give more accurate predictions on the training data,\\nbut it also starts to make more reasonable predictions on text that it's never\\nseen before.\\nGiven the huge number of parameters and the enormous amount of training data,\\nthe scale of computation involved in training a large language model is mind-boggling.\\nTo illustrate, imagine that you could perform one\\nbillion additions and multiplications every single second.\\nHow long do you think it would take for you to do all of the\\noperations involved in training the largest language models?\\nDo you think it would take a year?\\nMaybe something like 10,000 years?\\nThe answer is actually much more than that.\\nIt's well over 100 million years.\\nThis is only part of the story, though.\\nThis whole process is called pre-training.\\nThe goal of auto-completing a random passage of text from the\\ninternet is very different from the goal of being a good AI assistant.\\nTo address this, chatbots undergo another type of training,\\njust as important, called reinforcement learning with human feedback.\\nWorkers flag unhelpful or problematic predictions,\\nand their corrections further change the model's parameters,\\nmaking them more likely to give predictions that users prefer.\\nLooking back at the pre-training, though, this staggering amount of\\ncomputation is only made possible by using special computer chips that\\nare optimized for running many operations in parallel, known as GPUs.\\nHowever, not all language models can be easily parallelized.\\nPrior to 2017, most language models would process text one word at a time,\\nbut then a team of researchers at Google introduced a new model known as the transformer.\\nTransformers don't read text from the start to the finish,\\nthey soak it all in at once, in parallel.\\nThe very first step inside a transformer, and most other language models for that matter,\\nis to associate each word with a long list of numbers.\\nThe reason for this is that the training process only works with continuous values,\\nso you have to somehow encode language using numbers,\\nand each of these lists of numbers may somehow encode the meaning of the\\ncorresponding word.\\nWhat makes transformers unique is their reliance\\non a special operation known as attention.\\nThis operation gives all of these lists of numbers a chance to talk to one another\\nand refine the meanings they encode based on the context around, all done in parallel.\\nFor example, the numbers encoding the word bank might be changed based on the\\ncontext surrounding it to somehow encode the more specific notion of a riverbank.\\nTransformers typically also include a second type of operation known\\nas a feed-forward neural network, and this gives the model extra\\ncapacity to store more patterns about language learned during training.\\nAll of this data repeatedly flows through many different iterations of\\nthese two fundamental operations, and as it does so,\\nthe hope is that each list of numbers is enriched to encode whatever\\ninformation might be needed to make an accurate prediction of what word\\nfollows in the passage.\\nAt the end, one final function is performed on the last vector in this sequence,\\nwhich now has had a chance to be influenced by all the other context from the input text,\\nas well as everything the model learned during training,\\nto produce a prediction of the next word.\\nAgain, the model's prediction looks like a probability for every possible next word.\\nAlthough researchers design the framework for how each of these steps work,\\nit's important to understand that the specific behavior is an emergent phenomenon\\nbased on how those hundreds of billions of parameters are tuned during training.\\nThis makes it incredibly challenging to determine\\nwhy the model makes the exact predictions that it does.\\nWhat you can see is that when you use large language model predictions to autocomplete\\na prompt, the words that it generates are uncannily fluent, fascinating, and even useful.\\nIf you're a new viewer and you're curious about more details on how\\ntransformers and attention work, boy do I have some material for you.\\nOne option is to jump into a series I made about deep learning,\\nwhere we visualize and motivate the details of attention and all the other steps\\nin a transformer.\\nAlso, on my second channel I just posted a talk I gave a couple\\nmonths ago about this topic for the company TNG in Munich.\\nSometimes I actually prefer the content I make as a casual talk rather than a produced\\nvideo, but I leave it up to you which one of these feels like the better follow-on.\"]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcript"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c64def",
   "metadata": {},
   "source": [
    "## Text Splitting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "206b802e",
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = RecursiveCharacterTextSplitter(\n",
    "\tchunk_size=1000,\n",
    "\tchunk_overlap=200,\n",
    ")\n",
    "chunks = splitter.create_documents(transcript)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d40b4960",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f109644",
   "metadata": {},
   "source": [
    "## Embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aded4f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding= GoogleGenerativeAIEmbeddings(model=\"gemini-embedding-001\")\n",
    "\n",
    "vector_store = FAISS.from_documents(chunks, embedding, ids=[str(i) for i in range(len(chunks))])\n",
    "vector_store.save_local(\"youtube_transcript_vector_store\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cac90f9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '0',\n",
       " 1: '1',\n",
       " 2: '2',\n",
       " 3: '3',\n",
       " 4: '4',\n",
       " 5: '5',\n",
       " 6: '6',\n",
       " 7: '7',\n",
       " 8: '8',\n",
       " 9: '9'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_store.index_to_docstore_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51ac461",
   "metadata": {},
   "source": [
    "## Retrival\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e92a653a",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriver = vector_store.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 5})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "406fda67",
   "metadata": {},
   "source": [
    "## Augumentation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0657ea7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9f7620b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(\n",
    "\ttemplate=\"\"\"\n",
    "\tYou are a helpful assistant.\n",
    "        Answer ONLY from the provided transcript context.\n",
    "        If the context is insufficient, just say you don't know.\n",
    "\n",
    "        {context}\n",
    "        Question: {question}\n",
    " \"\"\",\n",
    "\tinput_variables=[\"context\", \"question\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4099157d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(retrieved_docs):\n",
    "    context_text = \"\\n\\n\".join(doc.page_content for doc in retrieved_docs)\n",
    "    return context_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7817f869",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ff2f439d",
   "metadata": {},
   "outputs": [],
   "source": [
    "parallel_chain = RunnableParallel({\n",
    "    'context': retriver | RunnableLambda(format_docs),\n",
    "    'question': RunnablePassthrough()\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "357af2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_chain = parallel_chain | prompt | llm | parser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32ec4d2",
   "metadata": {},
   "source": [
    "## Generation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0748a638",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The video discusses how large language models (LLMs) and transformers work. It explains that LLMs are sophisticated mathematical functions that predict the next word for any piece of text by assigning probabilities to all possible next words. This process is used to build chatbots, where the model repeatedly predicts the next word to complete a dialogue.\n",
      "\n",
      "Internally, transformers associate each word with a list of numbers, as the training process works with continuous values. A key feature of transformers is the \"attention\" operation, which allows these numbers to communicate and refine their encoded meanings based on context, all in parallel. They also include feed-forward neural networks.\n",
      "\n",
      "The transcript notes that while researchers design the framework, the specific behavior of LLMs is an emergent phenomenon from tuning billions of parameters during training, making it challenging to determine why exact predictions are made. LLMs are trained on enormous amounts of text, with an example given of GPT-3's training data taking over 2600 years for a human to read non-stop. The speaker also mentions having other resources, including a series on deep learning and a talk, for viewers interested in more details on transformers and attention.\n"
     ]
    }
   ],
   "source": [
    "result = main_chain.invoke('Can you summarize the video')\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "44e2af9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RLHF stands for reinforcement learning with human feedback. It is a type of training that chatbots undergo to address the difference between auto-completing random text and being a good AI assistant. In this process, workers flag unhelpful or problematic predictions, and their corrections further change the model's parameters, making them more likely to give predictions that users prefer.\n"
     ]
    }
   ],
   "source": [
    "res = main_chain.invoke('waht is RLHF ?')\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3397554c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention is a special operation that gives lists of numbers, which encode the meaning of words, a chance to talk to one another and refine the meanings they encode based on the surrounding context, all done in parallel.\n"
     ]
    }
   ],
   "source": [
    "res = main_chain.invoke('waht is Attention ?')\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b832d967",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
